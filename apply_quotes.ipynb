{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392aa37b",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m image_copy_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(image)      \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Convert RGB image to grayscale\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Identify faces in the webcam using haar cascade\u001b[39;00m\n\u001b[0;32m     67\u001b[0m faces \u001b[38;5;241m=\u001b[39m face_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, \u001b[38;5;241m1.3\u001b[39m, \u001b[38;5;241m5\u001b[39m)  \n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time \n",
    "import numpy as np\n",
    "from cnn_arch import *\n",
    "\n",
    "def apply_quotes(face_points, image_copy_1,image_name):\n",
    "\n",
    "    '''\n",
    "    Apply quotes to a person's face\n",
    "    Parameters:\n",
    "    --------------------\n",
    "    face_points: The predicted facial keypoints from the camera\n",
    "    image_copy_1: Copy of original image\n",
    "    Returns:\n",
    "    -------------\n",
    "    image_copy_1: quotes applied to copy of original image\n",
    "    '''\n",
    "\n",
    "    quotes = cv2.imread(\"images/\"+image_name, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    for i in range(len(face_points)):\n",
    "        # Get the width of filter depending on left and right eye brow point\n",
    "        # Adjust the size of the filter slightly above eyebrow points \n",
    "        filter_width = 1.1*(face_points[i][14]+15 - face_points[i][18]+15)\n",
    "        scale_factor = filter_width/animal_filter.shape[1]\n",
    "        sg = cv2.resize(animal_filter,None, fx=scale_factor, fy = scale_factor, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        width = sg.shape[1]\n",
    "        height = sg.shape[0]\n",
    "        \n",
    "        # top left corner of animal_filter: x coordinate = average x coordinate of eyes - width/2\n",
    "        # y coordinate = average y coordinate of eyes - height/2\n",
    "        x1 = int((face_points[i][2]+5 + face_points[i][0]+5)/2 - width/2)\n",
    "        x2 = x1 + width\n",
    "\n",
    "        y1 = int((face_points[i][3]-65 + face_points[i][1]-65)/2 - height/3)\n",
    "        y2 = y1 + height\n",
    "\n",
    "        # Create an alpha mask based on the transparency values\n",
    "        alpha_fil = np.expand_dims(sg[:, :, 3]/255.0, axis=-1)\n",
    "        alpha_face = 1.0 - alpha_fil\n",
    "        \n",
    "        # Take a weighted sum of the image and the animal filter using the alpha values and (1- alpha)\n",
    "        image_copy_1[y1:y2, x1:x2] = (alpha_fil * sg[:, :, :3] + alpha_face * image_copy_1[y1:y2, x1:x2])\n",
    "    \n",
    "    return image_copy_1\n",
    "\n",
    "# Load the model built in the previous step\n",
    "model = load_model('best_model')\n",
    "\n",
    "# Get frontal face haar cascade\n",
    "face_cascade = cv2.CascadeClassifier('cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Get webcam\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read data from the webcam\n",
    "    _, image = camera.read() \n",
    "    image_copy = np.copy(image)\n",
    "    image_copy_1 = np.copy(image)\n",
    "    image_copy_2 = np.copy(image)      \n",
    "    \n",
    "    # Convert RGB image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n",
    "    # Identify faces in the webcam using haar cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)  \n",
    "    faces_keypoints = []\n",
    "    \n",
    "    # Loop through faces\n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "        # Crop Faces\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "     \n",
    "        # Scale Faces to 96x96\n",
    "        scaled_face = cv2.resize(face, (96,96), 0, 0, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Normalize input images to be between 0 and 1\n",
    "        ip_image = scaled_face / 255\n",
    "\n",
    "        # Format image to be the correct shape for the model\n",
    "        ip_image = np.expand_dims(ip_image, axis = 0)\n",
    "        ip_image = np.expand_dims(ip_image, axis = -1)\n",
    "\n",
    "        # Use model to predict keypoints on image\n",
    "        face_points = model.predict(ip_image)[0]\n",
    "\n",
    "        # Adjust keypoints to coordinates of original image\n",
    "        face_points[0::2] = face_points[0::2] * w/2 + w/2 + x\n",
    "        face_points[1::2] = face_points[1::2] * h/2 + h/2 + y\n",
    "        faces_keypoints.append(face_points)\n",
    "        \n",
    "        # Plot facial keypoints on image\n",
    "        for point in range(15):\n",
    "            cv2.circle(image_copy, (face_points[2*point], face_points[2*point + 1]), 2, (255, 255, 0), -1)\n",
    "\n",
    "        happy = apply_filters(faces_keypoints, image_copy_1,\"happy.png\")\n",
    "        sad = apply_filters(faces_keypoints, image_copy_2,\"sad.png\")\n",
    "        \n",
    "        # Screen with the filter\n",
    "        cv2.imshow('Screen with filter',happy)  \n",
    "        cv2.imshow('Screen with filter quote',sad)        \n",
    "        # Screen with facial keypoints   \n",
    "        cv2.imshow('Screen with facial Keypoints predicted',image_copy)        \n",
    "           \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):   \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc71d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c471a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fd978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
